{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971d0f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Baseline Models - Fraud Detection\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Objective**: Develop and evaluate baseline fraud detection models\\n\",\n",
    "    \"**Author**: Data Science Team\\n\",\n",
    "    \"**Date**: 2025-10-17\\n\",\n",
    "    \"**Version**: 1.0\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Table of Contents\\n\",\n",
    "    \"1. [Setup and Data Preparation](#1-setup-and-data-preparation)\\n\",\n",
    "    \"2. [Data Preprocessing](#2-data-preprocessing)\\n\",\n",
    "    \"3. [Baseline Models](#3-baseline-models)\\n\",\n",
    "    \"4. [Model Evaluation](#4-model-evaluation)\\n\",\n",
    "    \"5. [Model Comparison](#5-model-comparison)\\n\",\n",
    "    \"6. [Results Export](#6-results-export)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Setup and Data Preparation\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\\n\",\n",
    "    \"from sklearn.preprocessing import StandardScaler, RobustScaler\\n\",\n",
    "    \"from sklearn.linear_model import LogisticRegression\\n\",\n",
    "    \"from sklearn.tree import DecisionTreeClassifier\\n\",\n",
    "    \"from sklearn.ensemble import RandomForestClassifier\\n\",\n",
    "    \"from sklearn.naive_bayes import GaussianNB\\n\",\n",
    "    \"from sklearn.svm import SVC\\n\",\n",
    "    \"from sklearn.metrics import (\\n\",\n",
    "    \"    classification_report, confusion_matrix, roc_auc_score, \\n\",\n",
    "    \"    roc_curve, precision_recall_curve, average_precision_score,\\n\",\n",
    "    \"    precision_score, recall_score, f1_score\\n\",\n",
    "    \")\\n\",\n",
    "    \"from imblearn.over_sampling import SMOTE\\n\",\n",
    "    \"from imblearn.under_sampling import RandomUnderSampler\\n\",\n",
    "    \"from imblearn.combine import SMOTEENN\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"ðŸ¤– Model Development Environment Ready!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load feature-engineered dataset\\n\",\n",
    "    \"df = pd.read_csv('../outputs/dataset_with_features.csv', parse_dates=['timestamp'])\\n\",\n",
    "    \"print(f\\\"ðŸ“Š Data loaded: {df.shape}\\\")\\n\",\n",
    "    \"print(f\\\"Fraud rate: {df['is_fraud'].mean():.2%}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load feature importance for feature selection\\n\",\n",
    "    \"feature_importance = pd.read_csv('../outputs/feature_importance.csv')\\n\",\n",
    "    \"print(f\\\"âœ… Feature importance loaded: {len(feature_importance)} features\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Data Preprocessing\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Select features for modeling\\n\",\n",
    "    \"feature_cols = [col for col in df.columns \\n\",\n",
    "    \"                if col not in ['user_id', 'merchant_id', 'timestamp', 'is_fraud', 'transaction_id']]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Use top features based on mutual information\\n\",\n",
    "    \"top_features = feature_importance.head(20)['feature'].tolist()\\n\",\n",
    "    \"available_features = [f for f in top_features if f in df.columns]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"ðŸ“‹ Feature Selection:\\\")\\n\",\n",
    "    \"print(f\\\"   â€¢ Total available features: {len(feature_cols)}\\\")\\n\",\n",
    "    \"print(f\\\"   â€¢ Selected top features: {len(available_features)}\\\")\\n\",\n",
    "    \"print(f\\\"   â€¢ Features: {available_features[:10]}...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Prepare features and target\\n\",\n",
    "    \"X = df[available_features].copy()\\n\",\n",
    "    \"y = df['is_fraud'].copy()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Handle missing values\\n\",\n",
    "    \"X = X.fillna(X.median())\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Handle infinite values\\n\",\n",
    "    \"X = X.replace([np.inf, -np.inf], np.nan)\\n\",\n",
    "    \"X = X.fillna(X.median())\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nðŸ“Š Dataset prepared:\\\")\\n\",\n",
    "    \"print(f\\\"   â€¢ Features shape: {X.shape}\\\")\\n\",\n",
    "    \"print(f\\\"   â€¢ Target shape: {y.shape}\\\")\\n\",\n",
    "    \"print(f\\\"   â€¢ Fraud rate: {y.mean():.2%}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Split the data\\n\",\n",
    "    \"X_train, X_test, y_train, y_test = train_test_split(\\n\",\n",
    "    \"    X, y, test_size=0.2, random_state=42, stratify=y\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"ðŸ“Š Data Split:\\\")\\n\",\n",
    "    \"print(f\\\"   â€¢ Training set: {X_train.shape} (fraud rate: {y_train.mean():.2%})\\\")\\n\",\n",
    "    \"print(f\\\"   â€¢ Test set: {X_test.shape} (fraud rate: {y_test.mean():.2%})\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Scale features\\n\",\n",
    "    \"scaler = RobustScaler()\\n\",\n",
    "    \"X_train_scaled = scaler.fit_transform(X_train)\\n\",\n",
    "    \"X_test_scaled = scaler.transform(X_test)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"âœ… Features scaled using RobustScaler\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Baseline Models\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Define baseline models\\n\",\n",
    "    \"models = {\\n\",\n",
    "    \"    'Logistic Regression': LogisticRegression(\\n\",\n",
    "    \"        random_state=42, \\n\",\n",
    "    \"        class_weight='balanced',\\n\",\n",
    "    \"        max_iter=1000\\n\",\n",
    "    \"    ),\\n\",\n",
    "    \"    'Decision Tree': DecisionTreeClassifier(\\n\",\n",
    "    \"        random_state=42,\\n\",\n",
    "    \"        class_weight='balanced',\\n\",\n",
    "    \"        max_depth=10\\n\",\n",
    "    \"    ),\\n\",\n",
    "    \"    'Random Forest': RandomForestClassifier(\\n\",\n",
    "    \"        random_state=42,\\n\",\n",
    "    \"        class_weight='balanced',\\n\",\n",
    "    \"        n_estimators=100,\\n\",\n",
    "    \"        max_depth=10\\n\",\n",
    "    \"    ),\\n\",\n",
    "    \"    'Naive Bayes': GaussianNB(),\\n\",\n",
    "    \"    'SVM': SVC(\\n\",\n",
    "    \"        random_state=42,\\n\",\n",
    "    \"        class_weight='balanced',\\n\",\n",
    "    \"        probability=True,\\n\",\n",
    "    \"        kernel='rbf'\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"ðŸ¤– Baseline models defined: {list(models.keys())}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Train and evaluate models\\n\",\n",
    "    \"results = {}\\n\",\n",
    "    \"trained_models = {}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Cross-validation setup\\n\",\n",
    "    \"cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"ðŸ”„ Training baseline models...\\\\n\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"for name, model in models.items():\\n\",\n",
    "    \"    print(f\\\"Training {name}...\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Use scaled features for models that benefit from scaling\\n\",\n",
    "    \"    if name in ['Logistic Regression', 'SVM', 'Naive Bayes']:\\n\",\n",
    "    \"        X_train_model = X_train_scaled\\n\",\n",
    "    \"        X_test_model = X_test_scaled\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        X_train_model = X_train\\n\",\n",
    "    \"        X_test_model = X_test\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Train model\\n\",\n",
    "    \"    model.fit(X_train_model, y_train)\\n\",\n",
    "    \"    trained_models[name] = model\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Make predictions\\n\",\n",
    "    \"    y_pred = model.predict(X_test_model)\\n\",\n",
    "    \"    y_pred_proba = model.predict_proba(X_test_model)[:, 1]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calculate metrics\\n\",\n",
    "    \"    precision = precision_score(y_test, y_pred)\\n\",\n",
    "    \"    recall = recall_score(y_test, y_pred)\\n\",\n",
    "    \"    f1 = f1_score(y_test, y_pred)\\n\",\n",
    "    \"    auc = roc_auc_score(y_test, y_pred_proba)\\n\",\n",
    "    \"    ap = average_precision_score(y_test, y_pred_proba)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Cross-validation scores\\n\",\n",
    "    \"    cv_auc = cross_val_score(model, X_train_model, y_train, cv=cv, scoring='roc_auc')\\n\",\n",
    "    \"    cv_f1 = cross_val_score(model, X_train_model, y_train, cv=cv, scoring='f1')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Store results\\n\",\n",
    "    \"    results[name] = {\\n\",\n",
    "    \"        'precision': precision,\\n\",\n",
    "    \"        'recall': recall,\\n\",\n",
    "    \"        'f1_score': f1,\\n\",\n",
    "    \"        'roc_auc': auc,\\n\",\n",
    "    \"        'avg_precision': ap,\\n\",\n",
    "    \"        'cv_auc_mean': cv_auc.mean(),\\n\",\n",
    "    \"        'cv_auc_std': cv_auc.std(),\\n\",\n",
    "    \"        'cv_f1_mean': cv_f1.mean(),\\n\",\n",
    "    \"        'cv_f1_std': cv_f1.std(),\\n\",\n",
    "    \"        'y_pred': y_pred,\\n\",\n",
    "    \"        'y_pred_proba': y_pred_proba\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"   âœ… {name}: AUC={auc:.3f}, F1={f1:.3f}, Precision={precision:.3f}, Recall={recall:.3f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nðŸŽ‰ All models trained successfully!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Model Evaluation\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Create results summary table\\n\",\n",
    "    \"results_df = pd.DataFrame({\\n\",\n",
    "    \"    'Model': list(results.keys()),\\n\",\n",
    "    \"    'Precision': [results[model]['precision'] for model in results.keys()],\\n\",\n",
    "    \"    'Recall': [results[model]['recall'] for model in results.keys()],\\n\",\n",
    "    \"    'F1-Score': [results[model]['f1_score'] for model in results.keys()],\\n\",\n",
    "    \"    'ROC-AUC': [results[model]['roc_auc'] for model in results.keys()],\\n\",\n",
    "    \"    'Avg Precision': [results[model]['avg_precision'] for model in results.keys()],\\n\",\n",
    "    \"    'CV AUC (Mean Â± Std)': [f\\\"{results[model]['cv_auc_mean']:.3f} Â± {results[model]['cv_auc_std']:.3f}\\\" \\n\",\n",
    "    \"                           for model in results.keys()]\\n\",\n",
    "    \"}).round(3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Sort by ROC-AUC\\n\",\n",
    "    \"results_df = results_df.sort_values('ROC-AUC', ascending=False).reset_index(drop=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"ðŸ“Š MODEL PERFORMANCE COMPARISON:\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*50)\\n\",\n",
    "    \"display(results_df)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Identify best model\\n\",\n",
    "    \"best_model_name = results_df.iloc[0]['Model']\\n\",\n",
    "    \"best_auc = results_df.iloc[0]['ROC-AUC']\\n\",\n",
    "    \"print(f\\\"\\\\nðŸ† Best performing model: {best_model_name} (AUC: {best_auc:.3f})\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Plot model comparison\\n\",\n",
    "    \"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Performance metrics comparison\\n\",\n",
    "    \"metrics_to_plot = ['Precision', 'Recall', 'F1-Score', 'ROC-AUC']\\n\",\n",
    "    \"x_pos = np.arange(len(results_df))\\n\",\n",
    "    \"\\n\",\n",
    "    \"for i, metric in enumerate(metrics_to_plot):\\n\",\n",
    "    \"    ax = [ax1, ax2, ax3, ax4][i]\\n\",\n",
    "    \"    bars = ax.bar(x_pos, results_df[metric], alpha=0.7, \\n\",\n",
    "    \"                  color=plt.cm.Set3(np.linspace(0, 1, len(results_df))))\\n\",\n",
    "    \"    ax.set_title(f'{metric} Comparison', fontweight='bold')\\n\",\n",
    "    \"    ax.set_xlabel('Models')\\n\",\n",
    "    \"    ax.set_ylabel(metric)\\n\",\n",
    "    \"    ax.set_xticks(x_pos)\\n\",\n",
    "    \"    ax.set_xticklabels(results_df['Model'], rotation=45, ha='right')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add value labels on bars\\n\",\n",
    "    \"    for bar, value in zip(bars, results_df[metric]):\\n\",\n",
    "    \"        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\\n\",\n",
    "    \"                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# ROC curves comparison\\n\",\n",
    "    \"plt.figure(figsize=(12, 8))\\n\",\n",
    "    \"\\n\",\n",
    "    \"colors = plt.cm.Set1(np.linspace(0, 1, len(results)))\\n\",\n",
    "    \"for i, (name, result) in enumerate(results.items()):\\n\",\n",
    "    \"    fpr, tpr, _ = roc_curve(y_test, result['y_pred_proba'])\\n\",\n",
    "    \"    auc_score = result['roc_auc']\\n\",\n",
    "    \"    plt.plot(fpr, tpr, color=colors[i], lw=2, \\n\",\n",
    "    \"             label=f'{name} (AUC = {auc_score:.3f})')\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', alpha=0.5)\\n\",\n",
    "    \"plt.xlim([0.0, 1.0])\\n\",\n",
    "    \"plt.ylim([0.0, 1.05])\\n\",\n",
    "    \"plt.xlabel('False Positive Rate', fontweight='bold')\\n\",\n",
    "    \"plt.ylabel('True Positive Rate', fontweight='bold')\\n\",\n",
    "    \"plt.title('ROC Curves Comparison', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"plt.legend(loc='lower right')\\n\",\n",
    "    \"plt.grid(True, alpha=0.3)\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Precision-Recall curves comparison\\n\",\n",
    "    \"plt.figure(figsize=(12, 8))\\n\",\n",
    "    \"\\n\",\n",
    "    \"for i, (name, result) in enumerate(results.items()):\\n\",\n",
    "    \"    precision_curve, recall_curve, _ = precision_recall_curve(y_test, result['y_pred_proba'])\\n\",\n",
    "    \"    ap_score = result['avg_precision']\\n\",\n",
    "    \"    plt.plot(recall_curve, precision_curve, color=colors[i], lw=2,\\n\",\n",
    "    \"             label=f'{name} (AP = {ap_score:.3f})')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Baseline (random classifier)\\n\",\n",
    "    \"no_skill = len(y_test[y_test == 1]) / len(y_test)\\n\",\n",
    "    \"plt.plot([0, 1], [no_skill, no_skill], linestyle='--', color='navy', alpha=0.5, label='No Skill')\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.xlim([0.0, 1.0])\\n\",\n",
    "    \"plt.ylim([0.0, 1.05])\\n\",\n",
    "    \"plt.xlabel('Recall', fontweight='bold')\\n\",\n",
    "    \"plt.ylabel('Precision', fontweight='bold')\\n\",\n",
    "    \"plt.title('Precision-Recall Curves Comparison', fontsize=14, fontweight='bold')\\n\",\n",
    "    \"plt.legend(loc='upper right')\\n\",\n",
    "    \"plt.grid(True, alpha=0.3)\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Detailed evaluation of best model\\n\",\n",
    "    \"print(f\\\"\\\\nðŸ” DETAILED EVALUATION - {best_model_name}\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*50)\\n\",\n",
    "    \"\\n\",\n",
    "    \"best_result = results[best_model_name]\\n\",\n",
    "    \"y_pred_best = best_result['y_pred']\\n\",\n",
    "    \"y_pred_proba_best = best_result['y_pred_proba']\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Classification report\\n\",\n",
    "    \"print(\\\"\\\\nClassification Report:\\\")\\n\",\n",
    "    \"print(classification_report(y_test, y_pred_best))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Confusion matrix\\n\",\n",
    "    \"cm = confusion_matrix(y_test, y_pred_best)\\n\",\n",
    "    \"plt.figure(figsize=(8, 6))\\n\",\n",
    "    \"sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \\n\",\n",
    "    \"            xticklabels=['Legitimate', 'Fraud'],\\n\",\n",
    "    \"            yticklabels=['Legitimate', 'Fraud'])\\n\",\n",
    "    \"plt.title(f'Confusion Matrix - {best_model_name}', fontweight='bold')\\n\",\n",
    "    \"plt.ylabel('Actual')\\n\",\n",
    "    \"plt.xlabel('Predicted')\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Calculate business metrics\\n\",\n",
    "    \"tn, fp, fn, tp = cm.ravel()\\n\",\n",
    "    \"print(f\\\"\\\\nðŸ“Š Confusion Matrix Breakdown:\\\")\\n\",\n",
    "    \"print(f\\\"   â€¢ True Negatives (TN): {tn:,}\\\")\\n\",\n",
    "    \"print(f\\\"   â€¢ False Positives (FP): {fp:,}\\\")\\n\",\n",
    "    \"print(f\\\"   â€¢ False Negatives (FN): {fn:,}\\\")\\n\",\n",
    "    \"print(f\\\"   â€¢ True Positives (TP): {tp:,}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Business impact metrics\\n\",\n",
    "    \"fraud_test = y_test.sum()\\n\",\n",
    "    \"fraud_caught = tp\\n\",\n",
    "    \"fraud_missed = fn\\n\",\n",
    "    \"false_alarms = fp\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nðŸ’¼ Business Impact:\\\")\\n\",\n",
    "    \"print(f\\\"   â€¢ Total fraud cases in test: {fraud_test:,}\\\")\\n\",\n",
    "    \"print(f\\\"   â€¢ Fraud cases caught: {fraud_caught:,} ({fraud_caught/fraud_test:.1%})\\\")\\n\",\n",
    "    \"print(f\\\"   â€¢ Fraud cases missed: {fraud_missed:,} ({fraud_missed/fraud_test:.1%})\\\")\\n\",\n",
    "    \"print(f\\\"   â€¢ False alarms: {false_alarms:,}\\\")\\n\",\n",
    "    \"print(f\\\"   â€¢ False alarm rate: {false_alarms/(tn+fp):.2%}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Model Comparison\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Feature importance for tree-based models\\n\",\n",
    "    \"tree_models = ['Decision Tree', 'Random Forest']\\n\",\n",
    "    \"fig, axes = plt.subplots(1, len(tree_models), figsize=(16, 6))\\n\",\n",
    "    \"\\n\",\n",
    "    \"if len(tree_models) == 1:\\n\",\n",
    "    \"    axes = [axes]\\n\",\n",
    "    \"\\n\",\n",
    "    \"for i, model_name in enumerate(tree_models):\\n\",\n",
    "    \"    if model_name in trained_models:\\n\",\n",
    "    \"        model = trained_models[model_name]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Get feature importance\\n\",\n",
    "    \"        if hasattr(model, 'feature_importances_'):\\n\",\n",
    "    \"            importance = model.feature_importances_\\n\",\n",
    "    \"            feature_names = available_features\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Create importance DataFrame\\n\",\n",
    "    \"            importance_df = pd.DataFrame({\\n\",\n",
    "    \"                'feature': feature_names,\\n\",\n",
    "    \"                'importance': importance\\n\",\n",
    "    \"            }).sort_values('importance', ascending=True)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Plot top 15 features\\n\",\n",
    "    \"            top_features_plot = importance_df.tail(15)\\n\",\n",
    "    \"            axes[i].barh(range(len(top_features_plot)), top_features_plot['importance'])\\n\",\n",
    "    \"            axes[i].set_yticks(range(len(top_features_plot)))\\n\",\n",
    "    \"            axes[i].set_yticklabels(top_features_plot['feature'])\\n\",\n",
    "    \"            axes[i].set_xlabel('Feature Importance')\\n\",\n",
    "    \"            axes[i].set_title(f'{model_name} - Top Features', fontweight='bold')\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Prediction threshold analysis for best model\\n\",\n",
    "    \"print(f\\\"\\\\nðŸŽ¯ THRESHOLD ANALYSIS - {best_model_name}\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*40)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Test different thresholds\\n\",\n",
    "    \"thresholds = np.arange(0.1, 0.9, 0.1)\\n\",\n",
    "    \"threshold_results = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"for threshold in thresholds:\\n\",\n",
    "    \"    y_pred_thresh = (y_pred_proba_best >= threshold).astype(int)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    precision = precision_score(y_test, y_pred_thresh)\\n\",\n",
    "    \"    recall = recall_score(y_test, y_pred_thresh)\\n\",\n",
    "    \"    f1 = f1_score(y_test, y_pred_thresh)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calculate confusion matrix for business metrics\\n\",\n",
    "    \"    cm_thresh = confusion_matrix(y_test, y_pred_thresh)\\n\",\n",
    "    \"    tn_t, fp_t, fn_t, tp_t = cm_thresh.ravel()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    threshold_results.append({\\n\",\n",
    "    \"        'threshold': threshold,\\n\",\n",
    "    \"        'precision': precision,\\n\",\n",
    "    \"        'recall': recall,\\n\",\n",
    "    \"        'f1_score': f1,\\n\",\n",
    "    \"        'tp': tp_t,\\n\",\n",
    "    \"        'fp': fp_t,\\n\",\n",
    "    \"        'fn': fn_t,\\n\",\n",
    "    \"        'tn': tn_t\\n\",\n",
    "    \"    })\\n\",\n",
    "    \"\\n\",\n",
    "    \"threshold_df = pd.DataFrame(threshold_results)\\n\",\n",
    "    \"display(threshold_df.round(3))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot threshold analysis\\n\",\n",
    "    \"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Precision-Recall vs Threshold\\n\",\n",
    "    \"ax1.plot(threshold_df['threshold'], threshold_df['precision'], 'o-', label='Precision', linewidth=2)\\n\",\n",
    "    \"ax1.plot(threshold_df['threshold'], threshold_df['recall'], 's-', label='Recall', linewidth=2)\\n\",\n",
    "    \"ax1.plot(threshold_df['threshold'], threshold_df['f1_score'], '^-', label='F1-Score', linewidth=2)\\n\",\n",
    "    \"ax1.set_xlabel('Threshold')\\n\",\n",
    "    \"ax1.set_ylabel('Score')\\n\",\n",
    "    \"ax1.set_title('Performance vs Threshold', fontweight='bold')\\n\",\n",
    "    \"ax1.legend()\\n\",\n",
    "    \"ax1.grid(True, alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# False Positives vs True Positives\\n\",\n",
    "    \"ax2.plot(threshold_df['fp'], threshold_df['tp'], 'o-', linewidth=2, markersize=8)\\n\",\n",
    "    \"ax2.set_xlabel('False Positives')\\n\",\n",
    "    \"ax2.set_ylabel('True Positives')\\n\",\n",
    "    \"ax2.set_title('True Positives vs False Positives', fontweight='bold')\\n\",\n",
    "    \"ax2.grid(True, alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Annotate points with thresholds\\n\",\n",
    "    \"for i, row in threshold_df.iterrows():\\n\",\n",
    "    \"    ax2.annotate(f'{row[\\\"threshold\\\"]:.1f}', \\n\",\n",
    "    \"                (row['fp'], row['tp']), \\n\",\n",
    "    \"                xytext=(5, 5), textcoords='offset points')\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Recommend optimal threshold\\n\",\n",
    "    \"optimal_threshold_idx = threshold_df['f1_score'].idxmax()\\n\",\n",
    "    \"optimal_threshold = threshold_df.iloc[optimal_threshold_idx]\\n\",\n",
    "    \"print(f\\\"\\\\nðŸŽ¯ Recommended threshold: {optimal_threshold['threshold']:.1f}\\\")\\n\",\n",
    "    \"print(f\\\"   â€¢ F1-Score: {optimal_threshold['f1_score']:.3f}\\\")\\n\",\n",
    "    \"print(f\\\"   â€¢ Precision: {optimal_threshold['precision']:.3f}\\\")\\n\",\n",
    "    \"print(f\\\"   â€¢ Recall: {optimal_threshold['recall']:.3f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 6. Results Export\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Export results\\n\",\n",
    "    \"print(\\\"ðŸ’¾ EXPORTING BASELINE MODEL RESULTS:\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*40)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save model performance results\\n\",\n",
    "    \"results_df.to_csv('../outputs/baseline_model_results.csv', index=False)\\n\",\n",
    "    \"print(f\\\"âœ… Model results saved: ../outputs/baseline_model_results.csv\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save threshold analysis\\n\",\n",
    "    \"threshold_df.to_csv('../outputs/threshold_analysis.csv', index=False)\\n\",\n",
    "    \"print(f\\\"âœ… Threshold analysis saved: ../outputs/threshold_analysis.csv\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save best model predictions\\n\",\n",
    "    \"test_predictions = pd.DataFrame({\\n\",\n",
    "    \"    'y_true': y_test,\\n\",\n",
    "    \"    'y_pred': y_pred_best,\\n\",\n",
    "    \"    'y_pred_proba': y_pred_proba_best\\n\",\n",
    "    \"})\\n\",\n",
    "    \"test_predictions.to_csv('../outputs/best_model_predictions.csv', index=False)\\n\",\n",
    "    \"print(f\\\"âœ… Best model predictions saved: ../outputs/best_model_predictions.csv\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create summary report\\n\",\n",
    "    \"summary_report = f\\\"\\\"\\\"\\n\",\n",
    "    \"FRAUD DETECTION BASELINE MODELS - SUMMARY REPORT\\n\",\n",
    "    \"================================================\\n\",\n",
    "    \"\\n\",\n",
    "    \"Dataset Information:\\n\",\n",
    "    \"- Total samples: {len(X):,}\\n\",\n",
    "    \"- Training samples: {len(X_train):,}\\n\",\n",
    "    \"- Test samples: {len(X_test):,}\\n\",\n",
    "    \"- Features used: {len(available_features)}\\n\",\n",
    "    \"- Fraud rate: {y.mean():.2%}\\n\",\n",
    "    \"\\n\",\n",
    "    \"Best Performing Model:\\n\",\n",
    "    \"- Model: {best_model_name}\\n\",\n",
    "    \"- ROC-AUC: {best_auc:.3f}\\n\",\n",
    "    \"- Precision: {results[best_model_name]['precision']:.3f}\\n\",\n",
    "    \"- Recall: {results[best_model_name]['recall']:.3f}\\n\",\n",
    "    \"- F1-Score: {results[best_model_name]['f1_score']:.3f}\\n\",\n",
    "    \"\\n\",\n",
    "    \"Business Impact (Test Set):\\n\",\n",
    "    \"- Total fraud cases: {fraud_test:,}\\n\",\n",
    "    \"- Fraud cases detected: {fraud_caught:,} ({fraud_caught/fraud_test:.1%})\\n\",\n",
    "    \"- Fraud cases missed: {fraud_missed:,} ({fraud_missed/fraud_test:.1%})\\n\",\n",
    "    \"- False alarms: {false_alarms:,}\\n\",\n",
    "    \"- False alarm rate: {false_alarms/(tn+fp):.2%}\\n\",\n",
    "    \"\\n\",\n",
    "    \"Optimal Threshold:\\n\",\n",
    "    \"- Threshold: {optimal_threshold['threshold']:.1f}\\n\",\n",
    "    \"- F1-Score: {optimal_threshold['f1_score']:.3f}\\n\",\n",
    "    \"- Precision: {optimal_threshold['precision']:.3f}\\n\",\n",
    "    \"- Recall: {optimal_threshold['recall']:.3f}\\n\",\n",
    "    \"\\n\",\n",
    "    \"Next Steps:\\n\",\n",
    "    \"1. Implement ensemble models (XGBoost, LightGBM)\\n\",\n",
    "    \"2. Try neural networks and deep learning approaches\\n\",\n",
    "    \"3. Experiment with anomaly detection methods\\n\",\n",
    "    \"4. Apply advanced feature engineering techniques\\n\",\n",
    "    \"5. Handle class imbalance with SMOTE or cost-sensitive learning\\n\",\n",
    "    \"\\\"\\\"\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"with open('../outputs/baseline_models_summary.txt', 'w') as f:\\n\",\n",
    "    \"    f.write(summary_report)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"âœ… Summary report saved: ../outputs/baseline_models_summary.txt\\\")\\n\",\n",
    "    \"print(f\\\"\\\\nðŸŽ‰ Baseline model development completed successfully!\\\")\\n\",\n",
    "    \"print(f\\\"\\\\nðŸ“Š Key Findings:\\\")\\n\",\n",
    "    \"print(f\\\"   â€¢ Best model: {best_model_name} (AUC: {best_auc:.3f})\\\")\\n\",\n",
    "    \"print(f\\\"   â€¢ {fraud_caught}/{fraud_test} fraud cases detected ({fraud_caught/fraud_test:.1%})\\\")\\n\",\n",
    "    \"print(f\\\"   â€¢ {false_alarms:,} false alarms ({false_alarms/(tn+fp):.2%} FP rate)\\\")\\n\",\n",
    "    \"print(f\\\"   â€¢ Recommended threshold: {optimal_threshold['threshold']:.1f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Summary\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Models Evaluated\\n\",\n",
    "    \"1. **Logistic Regression**: Linear baseline with class balancing\\n\",\n",
    "    \"2. **Decision Tree**: Interpretable non-linear model\\n\",\n",
    "    \"3. **Random Forest**: Ensemble method with feature importance\\n\",\n",
    "    \"4. **Naive Bayes**: Probabilistic classifier\\n\",\n",
    "    \"5. **SVM**: Support vector machine with RBF kernel\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Key Findings\\n\",\n",
    "    \"- Best performing baseline model identified\\n\",\n",
    "    \"- Comprehensive evaluation across multiple metrics\\n\",\n",
    "    \"- Threshold optimization for business requirements\\n\",\n",
    "    \"- Feature importance analysis for interpretability\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Performance Insights\\n\",\n",
    "    \"- Class imbalance significantly impacts model performance\\n\",\n",
    "    \"- Tree-based models show good interpretability\\n\",\n",
    "    \"- Scaling benefits certain algorithms (Logistic Regression, SVM)\\n\",\n",
    "    \"- Cross-validation confirms model stability\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Next Steps\\n\",\n",
    "    \"1. **Advanced Models**: XGBoost, LightGBM, Neural Networks\\n\",\n",
    "    \"2. **Class Imbalance**: SMOTE, ADASYN, cost-sensitive learning\\n\",\n",
    "    \"3. **Feature Engineering**: Polynomial features, interaction terms\\n\",\n",
    "    \"4. **Hyperparameter Tuning**: Grid search and Bayesian optimization\\n\",\n",
    "    \"5. **Ensemble Methods**: Stacking and blending approaches\\n\",\n",
    "    \"\\n\",\n",
    "    \"---\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Next Notebook**: `02_ensemble_models.ipynb` - Advanced ensemble methods for improved performance\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3 (ipykernel)\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.12.0\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
